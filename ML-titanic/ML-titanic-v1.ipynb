{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://blog.patricktriest.com/titanic-machine-learning-in-python/   and https://humansofdata.atlan.com/2016/07/machine-learning-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is intended to be an accessible introduction on how to use machine learning techniques for your projects and data sets, regardless of your major or disipline. \n",
    "\n",
    "For our introduction, we will use a dataset with passengers of the RMS Titanic. We will use an open data set with data on the passengers aboard the infamous doomed sea voyage of 1912. By examining factors such as class, sex, and age, we will experiment with different machine learning algorithms and build a program that can predict whether a given passenger would have survived this disaster.\n",
    "\n",
    "To get started, we have to first import a few important libraries that will help us along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, svm, tree, preprocessing, metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the Jupyter Notebook environment set up, we have to get the Titanic dataset. It is already included in the Jupyter folder where this Notebook is located, and if you want a copy you can also get it here: http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = pd.read_excel('./titanic3.xls', 'titanic3', index_col=None, na_values=['NA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out to see what is in the file by taking a look at the head()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                                             name     sex  \\\n",
       "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
       "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
       "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
       "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
       "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
       "\n",
       "       age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
       "0  29.0000      0      0   24160  211.3375       B5        S    2    NaN   \n",
       "1   0.9167      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
       "2   2.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "3  30.0000      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
       "4  25.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  \n",
       "3  Montreal, PQ / Chesterville, ON  \n",
       "4  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column heading variables have the following meanings:\n",
    "\n",
    "survival: Survival (0 = no; 1 = yes)\n",
    "class: Passenger class (1 = first; 2 = second; 3 = third)\n",
    "name: Name\n",
    "sex: Sex\n",
    "age: Age\n",
    "sibsp: Number of siblings/spouses aboard\n",
    "parch: Number of parents/children aboard\n",
    "ticket: Ticket number\n",
    "fare: Passenger fare\n",
    "cabin: Cabin\n",
    "embarked: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "boat: Lifeboat (if survived)\n",
    "body: Body number (if did not survive and body was recovered)\n",
    "\n",
    "\n",
    "We can now us pandas to look at some characteristics of the data. For instance, the percentage of those who survived.  (i.e., 0 = no; 1 = yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3819709702062643"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df['survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation shows that only 38% of the passengers survived. Not the best odds. The reason for this massive loss of life is that the Titanic was only carrying 20 lifeboats, which was not nearly enough for the 1,317 passengers and 885 crew members aboard. It seems unlikely that all of the passengers would have had equal chances at survival, so we will continue breaking down the data to examine the social dynamics that determined who got a place on a lifeboat and who did not.\n",
    "\n",
    "For example, you can look to see what percentage survived based on the \"passenger class\" (1 = first; 2 = second; 3 = third)(i.e., first class, second class, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pclass</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.619195</td>\n",
       "      <td>39.159918</td>\n",
       "      <td>0.436533</td>\n",
       "      <td>0.365325</td>\n",
       "      <td>87.508992</td>\n",
       "      <td>162.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.429603</td>\n",
       "      <td>29.506705</td>\n",
       "      <td>0.393502</td>\n",
       "      <td>0.368231</td>\n",
       "      <td>21.179196</td>\n",
       "      <td>167.387097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.255289</td>\n",
       "      <td>24.816367</td>\n",
       "      <td>0.568406</td>\n",
       "      <td>0.400564</td>\n",
       "      <td>13.302889</td>\n",
       "      <td>155.818182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        survived        age     sibsp     parch       fare        body\n",
       "pclass                                                                \n",
       "1       0.619195  39.159918  0.436533  0.365325  87.508992  162.828571\n",
       "2       0.429603  29.506705  0.393502  0.368231  21.179196  167.387097\n",
       "3       0.255289  24.816367  0.568406  0.400564  13.302889  155.818182"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.groupby('pclass').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start drawing some interesting insights from this data. For instance, passengers in first class had a 62% chance of survival, compared to a 25.5% chance for those in 3rd class. Additionally, the lower classes generally consisted of younger people, and the ticket prices for first class were predictably much higher than those for second and third class. The average ticket price for first class (£87.5) is equivalent to $13,487 in 2016.\n",
    "\n",
    "We can go a step further to look at the data by passenger class and gender (i.e., sex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">1</td>\n",
       "      <td>female</td>\n",
       "      <td>0.965278</td>\n",
       "      <td>37.037594</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>109.412385</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male</td>\n",
       "      <td>0.340782</td>\n",
       "      <td>41.029250</td>\n",
       "      <td>0.340782</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>69.888385</td>\n",
       "      <td>162.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">2</td>\n",
       "      <td>female</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>27.499191</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.650943</td>\n",
       "      <td>23.234827</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male</td>\n",
       "      <td>0.146199</td>\n",
       "      <td>30.815401</td>\n",
       "      <td>0.327485</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>19.904946</td>\n",
       "      <td>171.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">3</td>\n",
       "      <td>female</td>\n",
       "      <td>0.490741</td>\n",
       "      <td>22.185307</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>15.324250</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male</td>\n",
       "      <td>0.152130</td>\n",
       "      <td>25.962273</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.255578</td>\n",
       "      <td>12.415462</td>\n",
       "      <td>151.854167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               survived        age     sibsp     parch        fare        body\n",
       "pclass sex                                                                    \n",
       "1      female  0.965278  37.037594  0.555556  0.472222  109.412385         NaN\n",
       "       male    0.340782  41.029250  0.340782  0.279330   69.888385  162.828571\n",
       "2      female  0.886792  27.499191  0.500000  0.650943   23.234827   52.000000\n",
       "       male    0.146199  30.815401  0.327485  0.192982   19.904946  171.233333\n",
       "3      female  0.490741  22.185307  0.791667  0.731481   15.324250  183.000000\n",
       "       male    0.152130  25.962273  0.470588  0.255578   12.415462  151.854167"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_sex_grouping = titanic_df.groupby(['pclass','sex']).mean()\n",
    "class_sex_grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot this data on a chart using matplotlib (which we imported above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f20f6237fd0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_sex_grouping['survived'].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effectiveness of the second part of this “Women and children first” policy can be deduced by breaking down the survival rate by age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f20f61d3860>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "group_by_age = pd.cut(titanic_df[\"age\"], np.arange(0, 90, 10))\n",
    "age_grouping = titanic_df.groupby(group_by_age).mean()\n",
    "age_grouping['survived'].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With analysis above, we can draw some fairly straightforward conclusions from this data — being a woman, being in 1st class, and being a child were all factors that could boost your chances of survival during this disaster.\n",
    "\n",
    "Now that we have gotten to know our data a little, let's focus on what machine learning can do for us in this case where we are trying to understand a historical event in new ways.  Let’s say we wanted to predict whether a given passenger would survive the disaster. This is where machine learning comes in: we will build a program that learns from the sample data to predict whether a given passenger would survive.\n",
    "\n",
    "This concept of training the program based on a subset of data (i.e., training data) from our total dataset is essential to machine learning. We will take a randomly-selected subset of data (e.g., 80% of the total for example) that will be used to train the  program, and then use the results of that training to test how well the program predicts the correct results using the remainder of the data (i.e., testing data).\n",
    "\n",
    "Before we can feed our data set into a machine learning algorithm, however, we have to remove missing values and split it into training and test sets.\n",
    "\n",
    "If we perform a count of each column, we will see that much of the data on certain fields is missing. Most machine learning algorithms will have a difficult time handling missing values, so we will need to make sure that each row has a value for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass       1309\n",
       "survived     1309\n",
       "name         1309\n",
       "sex          1309\n",
       "age          1046\n",
       "sibsp        1309\n",
       "parch        1309\n",
       "ticket       1309\n",
       "fare         1308\n",
       "cabin         295\n",
       "embarked     1307\n",
       "boat          486\n",
       "body          121\n",
       "home.dest     745\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the rows are missing values for “boat” and “cabin”, so we will remove these columns from the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(['body','cabin','boat'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number of rows are also missing the “home.dest” field; here we could fill the missing values with “NA”. But, like “name” and “ticket”, the “home.dest” column consist of non-categorical string values (in other words, most rows have a distinct string; such as the unique name of the passenger). These are difficult to use in a classification algorithm, so we will drop them from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(['name','ticket','home.dest'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant number of rows are also missing an age value. We have seen above that age could have a significant effect on survival chances, so we will have to drop all of rows that are missing an age value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When we run the count command again, we can see that all remaining columns now contain the same number of values. You can give it a try below, do you remember how to get the count function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you get this:\n",
    "\n",
    "pclass       1043\n",
    "survived     1043\n",
    "name         1043\n",
    "sex          1043\n",
    "age          1043\n",
    "sibsp        1043\n",
    "parch        1043\n",
    "ticket       1043\n",
    "fare         1043\n",
    "embarked     1043\n",
    "home.dest    1043\n",
    "dtype: int64\n",
    "\n",
    "If you couldn't remember how to get the above data, look up to earlier steps.  If you can still can find it, highlight the blank area below with your mouse.\n",
    "\n",
    "titanic_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little more wrangling of the data we will be ready to go.\n",
    "\n",
    "The “sex” and “embarked” fields are both string values that correspond to categories (i.e “Male” and “Female”) so we will run each through a preprocessor. This preprocessor will convert these strings into integer keys, making it easier for the classification algorithms to find patterns. For instance, “Female” and “Male” will be converted to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_titanic_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.sex = le.fit_transform(processed_df.sex)\n",
    "    processed_df.embarked = le.fit_transform(processed_df.embarked)\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we used the preprocessing tools for the sklearn library that we imported at the beginning. Within the library are tools for encoding labels (e.g., sex, embarked) into integer values (e.g., 0 or 1).\n",
    "\n",
    "We can now use the function we just defined above in order to preprocess the titanic_df we cleaned up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = preprocess_titanic_df(titanic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we separate the data set into two arrays: “X” containing all of the values for each row besides “survived”, and “y” containing only the “survived” value for that row. In other words, \"X\" is an array with all the variables that we want to predict the outcome of \"y\" (or survival). The classification algorithms will compare the attribute values of “X” to the corresponding values of “y” to detect patterns in how different attributes values tend to affect the survival of a passenger. If you have taken a course in regression analysis, this may look familiar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(['survived'], axis=1).values\n",
    "y = processed_df['survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we break the “X” and “y” array into two parts each — a training set and a testing set. We will feed the training set into the classification algorithm to form a trained model. Once the model is formed, we will use it to classify the testing set, allowing us to determine the accuracy of the model. Here we have made a 20/80 split, such that 80% of the dataset will be used for training and 20% will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data cleaned and sorted into training data (80%) and testing data (20%), we are now ready to get started with the machine learning.\n",
    "\n",
    "We will start off with a simple decision tree classifier. A decision tree examines one variable at a time and splits into one of two branches based on the result of that value, at which point it does the same for the next variable. \n",
    "\n",
    "This is what a trained decision tree for the Titanic dataset looks like if we set the maximum number of levels to 3:\n",
    "\n",
    "INSERT IMAGE\n",
    "\n",
    "The tree first splits by sex, and then by class, since it has learned during the training phase that these are the two most important features for determining survival. The dark blue boxes indicate passengers who are likely to survive, and the dark orange boxes represent passengers who are almost certainly doomed. Interestingly, after splitting by class, the main deciding factor determining the survival of women is the ticket fare that they paid, while the deciding factor for men is their age (with children being much more likely to survive).\n",
    "\n",
    "To create this tree, we first initialize an instance of an untrained decision tree classifier. (Here we will set the maximum depth of the tree to 10). Next we “fit” this classifier to our training set, enabling it to learn about how different factors affect the survivability of a passenger. Now that the decision tree is ready, we can “score” it using our test data to determine how accurate it is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt = tree.DecisionTreeClassifier(max_depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we “fit” this classifier to our training set, enabling it to learn about how different factors affect the survivability of a passenger. Now that the decision tree is ready, we can “score” it using our test data to determine how accurate it is when it is applied to the test data (the 20% of the data we pulled out from the total for this purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7751196172248804"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dt.fit (X_train, y_train)\n",
    "clf_dt.score (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting reading, 0.7464, means that the model correctly predicted the survival of 75% of the test set. Not bad for our first model, and a fairly simple model at that. \n",
    "\n",
    "Before we do however, you might be thinking that the accuracy of the model could vary depending on which rows were selected for the training (80%) and test (20%) data sets. We will get around this problem by using a shuffle validator.  This will randomly split the data 20 times, in the example below, with 20% of the data going into the test data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_validator = ShuffleSplit(n_splits=20, train_size=0.2, test_size=.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing this shuffle validator as a parameter to the “cross_val_score” function, we can score our classifier against each of the different splits, and compute the average accuracy and standard deviation from the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(clf):\n",
    "    scores = cross_val_score(clf, X, y, cv=shuffle_validator)\n",
    "    print(\"Accuracy: %0.4f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we test our decision tree model (from above, clf_dt) we can see that doing the split 20 times, on average our accuracy was 76% (with a standard deliviation of .03, that is that majority of the interations were between 79% and 73%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7297 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful. We now have our first model, and we used it 20 times with different training and testing data sets (pulled randomly from the full data set), and it does pretty good. But we can do better!  And we have a function that we can use to test our different models using similar procedures.\n",
    "\n",
    "The next classifier model we want to test is a Random Forest classification algorithm.  Unlike a Decision Tree, the “Random Forest” classification algorithm will create a multitude of (generally very poor) trees for the data set using different random subsets of the input variables, and will return whichever prediction was returned by the most trees. This helps to avoid “overfitting”, a problem that occurs when a model is so tightly fitted to arbitrary correlations in the training data that it performs poorly on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7682 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "clf_rf = ske.RandomForestClassifier(n_estimators=50)\n",
    "test_classifier(clf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will try Gradient Boosting classification algorithm. The “Gradient Boosting” classification algorithm will generate many weak, shallow prediction trees and will combine, or “boost”, them into a strong model. This model performs very well on our data set, but has the drawback of being relatively slow and difficult to optimize, as the model construction happens sequentially so it cannot be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7804 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "clf_gb = ske.GradientBoostingClassifier(n_estimators=50)\n",
    "test_classifier(clf_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many (many) classification algorithms, but let's try one more. Voting classification algorithm can be used to apply multiple conceptually divergent classification models to the same data set and will return the majority vote from all of the classifiers. For instance, if the gradient boosting classifier predicts that a passenger will not survive, but the decision tree and random forest classifiers predict that they will live, the voting classifier will chose the latter. So we basically put our three previous algorithms into the new algorithm, and it uses the best of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7746 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "eclf = ske.VotingClassifier([('dt', clf_dt), ('rf', clf_rf), ('gb', clf_gb)])\n",
    "test_classifier(eclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have probably also heard about Deep Neural Networks (DNN) as a tool used in machine learning, especially for classifying pictures or words. DNNs are exciting advancements in machine learning, but they are overkill for a small data set like out Titanic example.  If you are curious, here is a tutorial where someone used a DNN, and as you will see at the end its accuracy was only about 70% (again, not enough data to fully train a useful DNN model): https://www.kaggle.com/tabora/titanic-tensorflow-canned-estimators-beginners\n",
    "\n",
    "The limited amount of data is also a limitation on the accuracy in our models above (e.g., decision tree, random forest, etc.). \n",
    "\n",
    "Given that the accuracy for all of our models is maxing out around 80%, it will be interesting to look at specific passengers for whom these classification algorithms are incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to prepare the test data set of the first 20 listed passengers for each class. We do this by taking the first 20 rows and then mergining those into a new data set (testing_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers_set_1 = titanic_df[titanic_df.pclass == 1].iloc[:20,:].copy()\n",
    "passengers_set_2 = titanic_df[titanic_df.pclass == 2].iloc[:20,:].copy()\n",
    "passengers_set_3 = titanic_df[titanic_df.pclass == 3].iloc[:20,:].copy()\n",
    "passenger_set = pd.concat([passengers_set_1,passengers_set_2,passengers_set_3])\n",
    "testing_set = preprocess_titanic_df(passenger_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a training data set using all those who are not in the testing set. And then we prepare the data as before based on the \"X\" factors we want to train in order to predict the outcome \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.concat([titanic_df, passenger_set]).drop_duplicates(keep=False)\n",
    "training_set = preprocess_titanic_df(training_set)\n",
    "\n",
    "X_train2 = training_set.drop(['survived'], axis=1).values\n",
    "y_train2 = training_set['survived'].values\n",
    "X_test2 = testing_set.drop(['survived'], axis=1).values\n",
    "y_test2 = testing_set['survived'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run our best performing model from above these new data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eclf.fit (X_train2, y_train2)\n",
    "eclf.score (X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs pretty well statistically, but let's see how that looks when it comes to which passengers did it predict correctly.  To do this we predict survived (or not survived) for the testing data set, and list all those passengers for whom the their survival (1 or 0) was not as predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.6500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.7875</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass  survived     sex   age  sibsp  parch      fare embarked\n",
       "2         1         0  female   2.0      1      2  151.5500        S\n",
       "4         1         0  female  25.0      1      2  151.5500        S\n",
       "5         1         1    male  48.0      0      0   26.5500        S\n",
       "14        1         1    male  80.0      0      0   30.0000        S\n",
       "20        1         1    male  37.0      1      1   52.5542        S\n",
       "336       2         1    male  32.0      1      0   26.0000        S\n",
       "601       3         0    male  13.0      0      2   20.2500        S\n",
       "603       3         1  female  35.0      1      1   20.2500        S\n",
       "605       3         1    male  25.0      0      0    7.6500        S\n",
       "606       3         1    male  20.0      0      0    7.9250        S\n",
       "613       3         1    male  26.0      0      0   18.7875        C"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = eclf.predict(X_test2)\n",
    "passenger_set[passenger_set.survived != prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows all of the passengers in our test data set whose survival (or lack thereof) was incorrectly classified by the model. \n",
    "\n",
    "Sometimes when you are dealing the data sets like this, the human side of the story can get lost beneath the complicated math and statistical analysis. By examining passengers for whom our classification model was incorrect, we can begin to uncover some of the most fascinating, and sometimes tragic, stories of humans defying the odds.\n",
    "\n",
    "For instance, the first two incorrectly classified passengers are all members of the Allison family (passengers in rows 2 and 4 from the initial dataframe), who perished even though the model predicted that they would survive. These first class passengers were very wealthy, as can be evidenced by their far-above-average ticket prices. For Betsy (age 25) and Loraine (age 2) in particular, not surviving is very surprising, considering that we found earlier that over 96% of first class women lived through the disaster. (note, we have taken the 'names' column out of the dataframe in order to do the machine learning, so we are matching names based on the row numbers).\n",
    "\n",
    "So what happened? A surprising amount of information on each Titanic passenger is available online; it turns out that the Allison family was unable to find their youngest son Trevor and was unwilling to evacuate the ship without him. Tragically, Trevor was already safe in a lifeboat with his nurse and was the only member of the Allison family to survive the sinking.\n",
    "\n",
    "As you can see, machine learning gives us tools for asking interesting questions about the data we have.\n",
    "\n",
    "Initially you may disappointed by the accuracy of our machine learning models maxing out at about 80% for this data set. It’s easy to forget that these data points each represent real people, each of whom found themselves stuck on a sinking ship without enough lifeboats. When we looked into data points for which our model was wrong, we can uncover incredible stories of human nature driving people to defy their logical fate. It is important to never lose sight of the human element when analyzing this type of data set. This principle will be especially important going forward, as machine learning is increasingly applied to human data sets by organizations such as insurance companies, big banks, and law enforcement agencies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
